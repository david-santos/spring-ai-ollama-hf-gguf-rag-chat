spring.application.name=spring-ai-ollama-hf-gguf-rag-chat

# Logging
logging.level.org.springframework.ai=DEBUG
logging.level.org.springframework.web.client=DEBUG

# Ollama chat model (Qwen3-8B-GGUF)
# https://huggingface.co/Qwen/Qwen3-8B-GGUF
spring.ai.ollama.chat.options.model=hf.co/Qwen/Qwen3-8B-GGUF
# https://docs.spring.io/spring-ai/reference/2.0/api/chat/ollama-chat.html#_thinking_mode_reasoning
#spring.ai.ollama.chat.options.think-option=true
spring.ai.ollama.chat.options.temperature=0.6
spring.ai.ollama.chat.options.top-p=0.95
spring.ai.ollama.chat.options.top-k=20
spring.ai.ollama.chat.options.min-p=0
spring.ai.ollama.chat.options.presence-penalty=1.5

# Ollama embedding model (Qwen3-Embedding-8B-GGUF)
# https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF
spring.ai.ollama.embedding.options.model=hf.co/Qwen/Qwen3-Embedding-8B-GGUF

# Ollama auto-pull and init
spring.ai.ollama.init.pull-model-strategy=always
spring.ai.ollama.init.timeout=15m
spring.ai.ollama.init.max-retries=2

# PGVector (configure to match your Docker Postgres)
spring.datasource.url=jdbc:postgresql://localhost:5432/postgres
spring.datasource.username=postgres
spring.datasource.password=postgres
spring.datasource.driver-class-name=org.postgresql.Driver

# PGVector store schema and dimensions (must match embedding model output)
# Qwen3-Embedding-8B-GGUF produces 4096-dimensional vectors
# HNSW index supports at most 2000 dimensions, so use NONE for 4096 (exact search, no index)
spring.ai.vectorstore.pgvector.initialize-schema=true
spring.ai.vectorstore.pgvector.dimensions=4096
spring.ai.vectorstore.pgvector.index-type=NONE
# One-time: set to true to drop and recreate the table if you get "expected 768 dimensions, not 4096"
# (table was created with wrong dimensions). Set back to false after the next successful startup.
spring.ai.vectorstore.pgvector.remove-existing-vector-store-table=true
